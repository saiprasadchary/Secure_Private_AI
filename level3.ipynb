{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8MQV-mYocqD"
      },
      "source": [
        "# Distributed Swarm Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ64pkvAmews"
      },
      "source": [
        "## Created DSL Model\n",
        "\n",
        "\n",
        "*  Defined Conv2D Architecture\n",
        "*  Defined PSO Hyperparamers -- w, c1 and c2 respectively\n",
        "*  Number of Agents/Clients Used = 5\n",
        "*  Initialized velocities\n",
        "*  Splitted dataset among agents\n",
        "*  Number of Epochs = 5\n",
        "\n",
        "\n",
        "**Imports**:\n",
        "\n",
        "\n",
        "1.   Tensorflow\n",
        "2.   Keras\n",
        "3.   Numpy\n",
        "4.   Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNtmJCpdoDjs"
      },
      "outputs": [],
      "source": [
        "# Authors: Sai Prasad, Purnima Sai Koumudi\n",
        "# Model saved time: 04/25\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NiHrlgBotKG"
      },
      "source": [
        "**Downloading the dataset (CIFAR 10)**\n",
        "\n",
        "Data Preprossing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmCA78z2oFTg"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr01rJ3ZowVJ"
      },
      "source": [
        "**CNN Model Architecture**\n",
        "\n",
        "Optimizers used: Adam\n",
        "\n",
        "Loss: Categorical Crossentropy\n",
        "\n",
        "Activation: RELU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fABrGfxXoGxs"
      },
      "outputs": [],
      "source": [
        "# Define the CNN model architecture\n",
        "def create_cnn_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
        "        Conv2D(32, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.4),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJzIm7foo7PY"
      },
      "source": [
        "**Initialized PSO Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYEC3ognoKcU"
      },
      "outputs": [],
      "source": [
        "global_model = create_cnn_model()\n",
        "\n",
        "# PSO parameters initialization\n",
        "w = 0.9  # Inertia weight\n",
        "c1, c2 = 0.5, 0.5  # Cognitive and social coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjSyKquWo-d4"
      },
      "source": [
        "**Defined Number of Agents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApfJuLO_oL-Y"
      },
      "outputs": [],
      "source": [
        "# Initialize agents\n",
        "num_agents = 5\n",
        "agents = [create_cnn_model() for _ in range(num_agents)]\n",
        "personal_best_weights = [agent.get_weights() for agent in agents]\n",
        "personal_best_loss = [float('inf')] * num_agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhJInD8_pJGI"
      },
      "source": [
        "**Initialized Velocities**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlIlP_ZaoN5d"
      },
      "outputs": [],
      "source": [
        "# Initialize velocities\n",
        "def initialize_velocity(models):\n",
        "    return [[np.zeros_like(weights) for weights in model.get_weights()] for model in models]\n",
        "\n",
        "velocity = initialize_velocity(agents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Fyt5F1pRjA"
      },
      "source": [
        "**Splitting the dataset among agents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlh28S8woPBn"
      },
      "outputs": [],
      "source": [
        "# Split dataset among agents\n",
        "indices = np.arange(len(x_train))\n",
        "np.random.shuffle(indices)\n",
        "x_train, y_train = x_train[indices], y_train[indices]\n",
        "splits = np.array_split(indices, num_agents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHk8fgRPpWLs"
      },
      "source": [
        "**Training the model defined**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9X_dqEooSnT"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "epochs = 25\n",
        "last_global_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, agent in enumerate(agents):\n",
        "        idx = splits[i]\n",
        "        x_split, y_split = x_train[idx], y_train[idx]\n",
        "        x_agent_train, x_agent_val, y_agent_train, y_agent_val = train_test_split(x_split, y_split, test_size=0.1)\n",
        "\n",
        "        history = agent.fit(x_agent_train, y_agent_train, validation_data=(x_agent_val, y_agent_val), epochs=1, batch_size=32, verbose=0)\n",
        "        val_loss = history.history['val_loss'][0]\n",
        "\n",
        "        if val_loss < personal_best_loss[i]:\n",
        "            personal_best_loss[i] = val_loss\n",
        "            personal_best_weights[i] = agent.get_weights()\n",
        "\n",
        "        for weight_idx, (current_weights, v) in enumerate(zip(agent.get_weights(), velocity[i])):\n",
        "            r1, r2 = np.random.random(), np.random.random()\n",
        "            v = w * v + c1 * r1 * (personal_best_weights[i][weight_idx] - current_weights) + c2 * r2 * (global_model.get_weights()[weight_idx] - current_weights)\n",
        "            current_weights += v\n",
        "            velocity[i][weight_idx] = v\n",
        "\n",
        "        agent.set_weights([w + v for w, v in zip(agent.get_weights(), velocity[i])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PRP5p7pZ2h"
      },
      "source": [
        "**Updation of Global Best Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFq3dl9LjYGx",
        "outputId": "2827d2da-dbfd-48a6-dd05-8f19c2f912b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n",
            "Global Model - Epoch 1: Test Loss: 1.9542, Test Accuracy: 0.2202\n",
            "Global Model - Epoch 2: Test Loss: 1.8415, Test Accuracy: 0.2650\n",
            "Global Model - Epoch 3: Test Loss: 1.8009, Test Accuracy: 0.3131\n",
            "Global Model - Epoch 4: Test Loss: 1.7216, Test Accuracy: 0.3437\n",
            "Global Model - Epoch 5: Test Loss: 1.6270, Test Accuracy: 0.3822\n",
            "Global Model - Epoch 6: Test Loss: 1.5173, Test Accuracy: 0.4452\n",
            "Global Model - Epoch 7: Test Loss: 1.4405, Test Accuracy: 0.4675\n",
            "Global Model - Epoch 8: Test Loss: 1.4071, Test Accuracy: 0.4785\n",
            "Global Model - Epoch 9: Test Loss: 1.3661, Test Accuracy: 0.5047\n",
            "Global Model - Epoch 10: Test Loss: 1.2978, Test Accuracy: 0.5207\n",
            "Global Model - Epoch 11: Test Loss: 1.2978, Test Accuracy: 0.5207\n",
            "Global Model - Epoch 12: Test Loss: 1.2978, Test Accuracy: 0.5207\n",
            "Global Model - Epoch 13: Test Loss: 1.2798, Test Accuracy: 0.5370\n",
            "Global Model - Epoch 14: Test Loss: 1.2798, Test Accuracy: 0.5370\n",
            "Global Model - Epoch 15: Test Loss: 1.2798, Test Accuracy: 0.5370\n",
            "Global Model - Epoch 16: Test Loss: 1.2798, Test Accuracy: 0.5370\n",
            "Global Model - Epoch 17: Test Loss: 1.2307, Test Accuracy: 0.5572\n",
            "Global Model - Epoch 18: Test Loss: 1.1575, Test Accuracy: 0.5847\n",
            "Global Model - Epoch 19: Test Loss: 1.1970, Test Accuracy: 0.5693\n",
            "Global Model - Epoch 20: Test Loss: 1.1277, Test Accuracy: 0.5982\n",
            "Global Model - Epoch 21: Test Loss: 1.1480, Test Accuracy: 0.5946\n",
            "Global Model - Epoch 22: Test Loss: 1.1154, Test Accuracy: 0.6014\n",
            "Global Model - Epoch 23: Test Loss: 1.0930, Test Accuracy: 0.6107\n",
            "Global Model - Epoch 24: Test Loss: 1.0930, Test Accuracy: 0.6107\n",
            "Global Model - Epoch 25: Test Loss: 1.0930, Test Accuracy: 0.6107\n"
          ]
        }
      ],
      "source": [
        "# Update global model to the best performing agent's weights\n",
        "    best_agent_index = np.argmin(personal_best_loss)\n",
        "    global_model.set_weights(personal_best_weights[best_agent_index])\n",
        "    global_loss, global_accuracy = global_model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    if global_loss < last_global_loss:\n",
        "        last_global_loss = global_loss\n",
        "    else:\n",
        "        w *= 0.95  # Decrease inertia to reduce oscillations\n",
        "        c1 *= 1.05  # Increase cognitive exploration\n",
        "        c2 *= 1.05  # Increase social learning\n",
        "\n",
        "    print(f\"Global Model - Epoch {epoch+1}: Test Loss: {global_loss:.4f}, Test Accuracy: {global_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxgRcWgVpiGL"
      },
      "source": [
        "**Model Saving Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HDLd-C9h4kYA",
        "outputId": "7e151158-3e2f-44a0-d629-7050114d8355"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "global_model.save(\"trained_model_3.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
